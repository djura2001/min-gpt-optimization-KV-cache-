{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shows how one can generate text given a prompt and some hyperparameters, using either minGPT or huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aleksa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from mingpt.model import GPT\n",
    "from mingpt.utils import set_seed\n",
    "from mingpt.bpe import BPETokenizer\n",
    "set_seed(3407)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_mingpt = True # use minGPT or huggingface/transformers model?\n",
    "model_type = 'gpt2'\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 152.79M\n",
      "MinGPT model has: 221 parameters\n",
      "HuggingFace checkpoint has: 149 parameters\n",
      "Keys to copy: 149\n",
      "number of parameters: 152.79M\n",
      "MinGPT model has: 221 parameters\n",
      "HuggingFace checkpoint has: 149 parameters\n",
      "Keys to copy: 149\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttentionVanilla(\n",
       "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (attn_vanilla): CausalSelfAttentionVanilla(\n",
       "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModuleDict(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): NewGELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if use_mingpt:\n",
    "    model = GPT.from_pretrained(model_type,vanilla=False)\n",
    "    model_vanilla = GPT.from_pretrained(model_type, True)\n",
    "else:\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "    model.config.pad_token_id = model.config.eos_token_id # suppress a warning\n",
    "\n",
    "# ship model to device and set to eval mode\n",
    "model.to(device)\n",
    "model.eval();\n",
    "model_vanilla.to(device)\n",
    "model_vanilla.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate(prompt='', num_samples=10, steps=20, do_sample=True):\n",
    "        \n",
    "    # tokenize the input prompt into integer input sequence\n",
    "    if use_mingpt:\n",
    "        tokenizer = BPETokenizer()\n",
    "        if prompt == '':\n",
    "            # to create unconditional samples...\n",
    "            # manually create a tensor with only the special <|endoftext|> token\n",
    "            # similar to what openai's code does here https://github.com/openai/gpt-2/blob/master/src/generate_unconditional_samples.py\n",
    "            x = torch.tensor([[tokenizer.encoder.encoder['<|endoftext|>']]], dtype=torch.long)\n",
    "        else:\n",
    "            x = tokenizer(prompt).to(device)\n",
    "    else:\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(model_type)\n",
    "        if prompt == '': \n",
    "            # to create unconditional samples...\n",
    "            # huggingface/transformers tokenizer special cases these strings\n",
    "            prompt = '<|endoftext|>'\n",
    "        encoded_input = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "        x = encoded_input['input_ids']\n",
    "    \n",
    "    # we'll process all desired num_samples in a batch, so expand out the batch dim\n",
    "    x = x.expand(num_samples, -1)\n",
    "    model.reset_kv_cache()\n",
    "    # forward the model `steps` times to get samples, in a batch\n",
    "    y = model.generate(x, max_new_tokens=steps, do_sample=do_sample, top_k=40)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        \n",
    "        out = tokenizer.decode(y[i].cpu().squeeze())\n",
    "        print('-'*80)\n",
    "        print(out)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Michael Jordan is a, and the \"I was a, and the \"I was a, and the \"I\n",
      "--------------------------------------------------------------------------------\n",
      "Michael Jordan is a, and the \"I was a, and the \"I was a, and the \"I\n",
      "--------------------------------------------------------------------------------\n",
      "Michael Jordan is a, and the \"I was a, and the \"I was a, and the \"I\n",
      "--------------------------------------------------------------------------------\n",
      "Michael Jordan is a, and the \"I was a, and the \"I was a, and the \"I\n",
      "--------------------------------------------------------------------------------\n",
      "Michael Jordan is a, and the \"I was a, and the \"I was a, and the \"I\n",
      "--------------------------------------------------------------------------------\n",
      "Michael Jordan is a, and the \"I was a, and the \"I was a, and the \"I\n",
      "--------------------------------------------------------------------------------\n",
      "Michael Jordan is a, and the \"I was a, and the \"I was a, and the \"I\n",
      "--------------------------------------------------------------------------------\n",
      "Michael Jordan is a, and the \"I was a, and the \"I was a, and the \"I\n",
      "--------------------------------------------------------------------------------\n",
      "Michael Jordan is a, and the \"I was a, and the \"I was a, and the \"I\n",
      "--------------------------------------------------------------------------------\n",
      "Michael Jordan is a, and the \"I was a, and the \"I was a, and the \"I\n"
     ]
    }
   ],
   "source": [
    "generate(prompt='Michael Jordan is', num_samples=10, steps=20, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_vanilla(prompt='', num_samples=10, steps=20, do_sample=False):\n",
    "        \n",
    "    # tokenize the input prompt into integer input sequence\n",
    "    if use_mingpt:\n",
    "        tokenizer = BPETokenizer()\n",
    "        if prompt == '':\n",
    "            # to create unconditional samples...\n",
    "            # manually create a tensor with only the special <|endoftext|> token\n",
    "            # similar to what openai's code does here https://github.com/openai/gpt-2/blob/master/src/generate_unconditional_samples.py\n",
    "            x = torch.tensor([[tokenizer.encoder.encoder['<|endoftext|>']]], dtype=torch.long)\n",
    "        else:\n",
    "            x = tokenizer(prompt).to(device)\n",
    "    else:\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(model_type)\n",
    "        if prompt == '': \n",
    "            # to create unconditional samples...\n",
    "            # huggingface/transformers tokenizer special cases these strings\n",
    "            prompt = '<|endoftext|>'\n",
    "        encoded_input = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "        x = encoded_input['input_ids']\n",
    "    \n",
    "    # we'll process all desired num_samples in a batch, so expand out the batch dim\n",
    "    x = x.expand(num_samples, -1)\n",
    "    #model.reset_kv_cache()\n",
    "    # forward the model `steps` times to get samples, in a batch\n",
    "    y = model_vanilla.generate(x, max_new_tokens=steps, do_sample=do_sample, top_k=40)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        \n",
    "        out = tokenizer.decode(y[i].cpu().squeeze())\n",
    "        print('-'*80)\n",
    "        print(out)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Michael Jordan is a senior writer for ESPN The Magazine. Follow him on Twitter @JordanWWC.<|endoftext|>The U\n",
      "--------------------------------------------------------------------------------\n",
      "Michael Jordan is a senior writer for ESPN The Magazine. Follow him on Twitter @JordanWWC.<|endoftext|>The U\n",
      "--------------------------------------------------------------------------------\n",
      "Michael Jordan is a senior writer for ESPN The Magazine. Follow him on Twitter @JordanWWC.<|endoftext|>The U\n",
      "--------------------------------------------------------------------------------\n",
      "Michael Jordan is a senior writer for ESPN The Magazine. Follow him on Twitter @JordanWWC.<|endoftext|>The U\n",
      "--------------------------------------------------------------------------------\n",
      "Michael Jordan is a senior writer for ESPN The Magazine. Follow him on Twitter @JordanWWC.<|endoftext|>The U\n",
      "--------------------------------------------------------------------------------\n",
      "Michael Jordan is a senior writer for ESPN The Magazine. Follow him on Twitter @JordanWWC.<|endoftext|>The U\n",
      "--------------------------------------------------------------------------------\n",
      "Michael Jordan is a senior writer for ESPN The Magazine. Follow him on Twitter @JordanWWC.<|endoftext|>The U\n",
      "--------------------------------------------------------------------------------\n",
      "Michael Jordan is a senior writer for ESPN The Magazine. Follow him on Twitter @JordanWWC.<|endoftext|>The U\n",
      "--------------------------------------------------------------------------------\n",
      "Michael Jordan is a senior writer for ESPN The Magazine. Follow him on Twitter @JordanWWC.<|endoftext|>The U\n",
      "--------------------------------------------------------------------------------\n",
      "Michael Jordan is a senior writer for ESPN The Magazine. Follow him on Twitter @JordanWWC.<|endoftext|>The U\n"
     ]
    }
   ],
   "source": [
    "generate_vanilla(prompt='Michael Jordan is', num_samples=10, steps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(prompt='', num_samples=1, steps=20, do_sample=False):\n",
    "# tokenize the input prompt into integer input sequence\n",
    "    model = GPT.from_pretrained(model_type,vanilla=False)\n",
    "    model_vanilla = GPT.from_pretrained(model_type, True)\n",
    "    model.to(device)\n",
    "    model.eval();\n",
    "    model_vanilla.to(device)\n",
    "    model_vanilla.eval();\n",
    "    if use_mingpt:\n",
    "        tokenizer = BPETokenizer()\n",
    "        if prompt == '':\n",
    "            # to create unconditional samples...\n",
    "            # manually create a tensor with only the special <|endoftext|> token\n",
    "            # similar to what openai's code does here https://github.com/openai/gpt-2/blob/master/src/generate_unconditional_samples.py\n",
    "            x = torch.tensor([[tokenizer.encoder.encoder['<|endoftext|>']]], dtype=torch.long)\n",
    "        else:\n",
    "            x = tokenizer(prompt).to(device)\n",
    "    else:\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(model_type)\n",
    "        if prompt == '': \n",
    "            # to create unconditional samples...\n",
    "            # huggingface/transformers tokenizer special cases these strings\n",
    "            prompt = '<|endoftext|>'\n",
    "        encoded_input = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "        x = encoded_input['input_ids']\n",
    "    \n",
    "    # we'll process all desired num_samples in a batch, so expand out the batch dim\n",
    "    x = x.expand(num_samples, -1)\n",
    "\n",
    "    '''b, t = x.size()\n",
    "    num_tokens = t\n",
    "    kv_cached = False\n",
    "    assert t <= model.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n",
    "    if not kv_cached:\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n",
    "    else:\n",
    "        seq = num_tokens  # broj tokena do sada, od 1..N\n",
    "        pos_index = (seq - 1)   # da ne prelazi limit\n",
    "        pos = torch.tensor([[pos_index]], dtype=torch.long, device=device)\n",
    "    # forward the GPT model itself\n",
    "\n",
    "    tok_emb = model.transformer.wte(x) # token embeddings of shape (b, t, n_embd)\n",
    "    pos_emb = model.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n",
    "    x = model.transformer.drop(tok_emb + pos_emb)'''\n",
    "    '''x1 = model.transformer.h[0](x)\n",
    "    x2 = model_vanilla.transformer.h[0](x)'''\n",
    "    x1 = model.generate(x, max_new_tokens=5, do_sample=False, top_k=40)\n",
    "    x2 = model_vanilla.generate(x, max_new_tokens=5, do_sample=False, top_k=40)\n",
    "    return torch.allclose(x1,x2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 152.79M\n",
      "MinGPT model has: 221 parameters\n",
      "HuggingFace checkpoint has: 149 parameters\n",
      "Keys to copy: 149\n",
      "number of parameters: 152.79M\n",
      "MinGPT model has: 221 parameters\n",
      "HuggingFace checkpoint has: 149 parameters\n",
      "Keys to copy: 149\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(compare(prompt='Michael Jordan is'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
